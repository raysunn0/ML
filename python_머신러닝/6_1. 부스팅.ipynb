{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.ensemble.GradientBoostingClassifier\n",
    "* class sklearn.ensemble.GradientBoostingClassifier(*, loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "> https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feature_name_df = pd.read_csv('./Human_activity/features.txt', sep='\\s+', header=None, names=['column_index', 'column_name'])\n",
    "feature_name = feature_name_df.iloc[:, 1].values.tolist()\n",
    "feature_dup_df = feature_name_df.groupby('column_name').count()\n",
    "\n",
    "def get_new_feature_name_df(old_feature_name_df):\n",
    "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(), columns=['dup_cnt'])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')\n",
    "    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0] + '_' + str(x[1]) if x[1]>0 else x[0], axis=1)\n",
    "    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)\n",
    "    return new_feature_name_df\n",
    "    \n",
    "def get_human_dataset():\n",
    "    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당\n",
    "    feature_name_df = pd.read_csv('./Human_activity/features.txt', sep='\\s+', header=None, names=['column_index', 'column_name'])\n",
    "\n",
    "    # 중복된 피처명을 수정하는 get_new_feature_df()를 이용, 신규 피처명 DataFrame 생성\n",
    "    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n",
    "\n",
    "    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환\n",
    "    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n",
    "\n",
    "    # 학습 피처 데이터 셋과 테스트 피처 데이터를 DataFrame으로 로딩, 컬럼명은 feature_name 적용\n",
    "    X_train = pd.read_csv('./Human_activity/train/X_train.txt', sep='\\s+', names=feature_name)\n",
    "    X_test = pd.read_csv('./Human_activity/test/X_test.txt', sep='\\s+', names=feature_name)\n",
    "\n",
    "    # 학습 레이블과 테스트 레이블 데이터를 DataFrame으로 로딩, 컬럼명은 action으로 부여\n",
    "    y_train = pd.read_csv('./Human_activity/train/y_train.txt', sep='\\s+', header=None, names=['action'])\n",
    "    y_test = pd.read_csv('./Human_activity/test/y_test.txt', sep='\\s+', header=None, names=['action'])\n",
    "\n",
    "    # 로드된 학습/테스트용 DataFrame을 모두 반환\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:570: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM 정확도 : 0.9389209365456397\n",
      "GBM 수행 시간 : 1217.9504833221436 초\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n",
    "\n",
    "# GBM 수행 시간 측정을 위함. 시작 시간 설정.\n",
    "start_time = time.time()\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print(f'GBM 정확도 : {gb_accuracy}')\n",
    "print(f'GBM 수행 시간 : {time.time() - start_time} 초')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20분 걸리는 거 실화인지?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost\n",
    "* class xgboost.XGBRegressor(*, objective='reg:squarederror', **kwargs)\n",
    "> https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (177557653.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [15], line 13\u001b[1;36m\u001b[0m\n\u001b[1;33m    booster='gbtree' # 트리,회귀(gblinear) 트리가 항상\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# 반드시 튜닝해야할 파라미터는  min_child_weight / max_depth / gamma\n",
    "\n",
    "clf=xgb.XGBClassifier()   # 파라미터 넣어줌.   모델생성\n",
    "clf.fit()                 # 파라미터 넣어줌.   데이터학습.                      \n",
    "clf.coef_                 # gbtree는 볼수없음.\n",
    "evals_result=clf.evals_result()\n",
    "clf.feature_importances_\n",
    "clf.predict()                 #파라미터 넣어줌.  예측\n",
    "\n",
    "xgb.XGBClassifier(\n",
    "    # General Parameter\n",
    "    booster='gbtree' # 트리,회귀(gblinear) 트리가 항상 \n",
    "                     # 더 좋은 성능을 내기 때문에 수정할 필요없다고한다.\n",
    "    silent=True  # running message출력안한다.\n",
    "                 # 모델이 적합되는 과정을 이해하기위해선 False으로한다.\n",
    "    min_child_weight=10   # 값이 높아지면 under-fitting 되는 \n",
    "                          # 경우가 있다. CV를 통해 튜닝되어야 한다.\n",
    "    max_depth=8     # 트리의 최대 깊이를 정의함. \n",
    "                    # 루트에서 가장 긴 노드의 거리.\n",
    "                    # 8이면 중요변수에서 결론까지 변수가 9개거친다.\n",
    "                    # Typical Value는 3-10. \n",
    "    gamma =0    # 노드가 split 되기 위한 loss function의 값이\n",
    "                # 감소하는 최소값을 정의한다. gamma 값이 높아질 수록 \n",
    "                # 알고리즘은 보수적으로 변하고, loss function의 정의\n",
    "                #에 따라 적정값이 달라지기때문에 반드시 튜닝.\n",
    "    nthread =4    # XGBoost를 실행하기 위한 병렬처리(쓰레드)\n",
    "                  #갯수. 'n_jobs' 를 사용해라.\n",
    "    colsample_bytree=0.8   # 트리를 생성할때 훈련 데이터에서 \n",
    "                           # 변수를 샘플링해주는 비율. 보통0.6~0.9\n",
    "    colsample_bylevel=0.9  # 트리의 레벨별로 훈련 데이터의 \n",
    "                           #변수를 샘플링해주는 비율. 보통0.6~0.9\n",
    "    n_estimators =(int)   #부스트트리의 양\n",
    "                          # 트리의 갯수. \n",
    "    objective = 'reg:linear','binary:logistic','multi:softmax',\n",
    "                'multi:softprob'  # 4가지 존재.\n",
    "            # 회귀 경우 'reg', binary분류의 경우 'binary',\n",
    "            # 다중분류경우 'multi'- 분류된 class를 return하는 경우 'softmax'\n",
    "            # 각 class에 속할 확률을 return하는 경우 'softprob'\n",
    "    random_state =  # random number seed.\n",
    "                    # seed 와 동일.\n",
    ")\n",
    "\n",
    "\n",
    "XGBClassifier.fit(\n",
    "    X (array_like)     # Feature matrix ( 독립변수)\n",
    "                       # X_train\n",
    "    Y (array)          # Labels (종속변수)\n",
    "                       # Y_train\n",
    "    eval_set           # 빨리 끝나기 위해 검증데이터와 같이써야한다.  \n",
    "                       # =[(X_train,Y_train),(X_vld, Y_vld)]\n",
    "    eval_metric = 'rmse','error','mae','logloss','merror',\n",
    "                'mlogloss','auc'  \n",
    "              # validation set (검증데이터)에 적용되는 모델 선택 기준.\n",
    "              # 평가측정. \n",
    "              # 회귀 경우 rmse ,  분류 -error   이외의 옵션은 함수정의\n",
    "    early_stopping_rounds=100,20\n",
    "              # 100번,20번 반복동안 최대화 되지 않으면 stop\n",
    ")\n",
    "[출처] 파이썬 Scikit-Learn형식 XGBoost 파라미터|작성자 현무\n",
    "https://blog.naver.com/PostView.nhn?blogId=gustn3964&logNo=221431714122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1 2 3 4 5 6]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 선언 예시\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\YS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:575\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    574\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 575\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\YS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:1357\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1352\u001b[0m     expected_classes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_)\n\u001b[0;32m   1353\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1354\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m expected_classes\u001b[39m.\u001b[39mshape\n\u001b[0;32m   1355\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m==\u001b[39m expected_classes)\u001b[39m.\u001b[39mall()\n\u001b[0;32m   1356\u001b[0m ):\n\u001b[1;32m-> 1357\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1358\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1359\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected: \u001b[39m\u001b[39m{\u001b[39;00mexpected_classes\u001b[39m}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1360\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1364\u001b[0m \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1 2 3 4 5 6]"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 모델 선언 예시\n",
    "model = XGBClassifier(n_estimators=500, learning_rate=0.2, max_depth=4, random_state = 32)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.predict(X_test)\n",
    "#y_pred = model.predict(X_test) # 예측 라벨(0과 1로 예측)\n",
    "\n",
    "# 예측 라벨과 실제 라벨 사이의 정확도 측정\n",
    "#accuracy_score(y_pred, y_test) # 0.7847533632286996\n",
    "\n",
    "# model=XGBClassifier(booster='gbtree', \n",
    "#                     colsample_bylevel=0.9, \n",
    "#                     colsample_bytree=0.8, \n",
    "#                     gamma=0, \n",
    "#                     max_depth=8, \n",
    "#                     min_child_weight=3, \n",
    "#                     n_estimators=50, \n",
    "#                     nthread=4, \n",
    "#                     objective='binary:logistic', \n",
    "#                     random_state=2, \n",
    "#                     silent= True)\n",
    "# model.fit(train_X,train_Y, eval_set=[(val_X,val_Y)],\n",
    "#              early_stopping_rounds=50,verbos=5)\n",
    "# model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1 2 3 4 5 6]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [25], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     12\u001b[0m xg_clf \u001b[38;5;241m=\u001b[39m XGBClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mxg_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m xg_pred \u001b[38;5;241m=\u001b[39m xg_clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     15\u001b[0m xg_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, xg_pred)\n",
      "File \u001b[1;32mc:\\Users\\YS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:575\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    574\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 575\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\YS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:1357\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1352\u001b[0m     expected_classes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_)\n\u001b[0;32m   1353\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1354\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m expected_classes\u001b[39m.\u001b[39mshape\n\u001b[0;32m   1355\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m==\u001b[39m expected_classes)\u001b[39m.\u001b[39mall()\n\u001b[0;32m   1356\u001b[0m ):\n\u001b[1;32m-> 1357\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1358\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1359\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected: \u001b[39m\u001b[39m{\u001b[39;00mexpected_classes\u001b[39m}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1360\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1364\u001b[0m \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1 2 3 4 5 6]"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "pd.Series(y_train)\n",
    "pd.Series(y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n",
    "start_time = time.time()\n",
    "xg_clf = XGBClassifier(n_estimators=500, learning_rate=0.2, max_depth=4, random_state = 32)\n",
    "xg_clf.fit(X_train, y_train)\n",
    "xg_pred = xg_clf.predict(X_test)\n",
    "xg_accuracy = accuracy_score(y_test, xg_pred)\n",
    "print('XG 정확도: {0: .4f}'.format(xg_accuracy))\n",
    "print('XG 수행 시간: {0: .1f} 초'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9463861554122837"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "pd.Series(y_train)\n",
    "pd.Series(y_test)\n",
    "\n",
    "model = XGBClassifier(n_estimators=500, learning_rate=0.2, max_depth=4, random_state = 32)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test) # 예측 라벨(0과 1로 예측)\n",
    "# 예측 라벨과 실제 라벨 사이의 정확도 측정\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e205c5c3685dc83fb395c1e91e521dfba0cb0fb0c3eaf2931f4234d8709f72ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
